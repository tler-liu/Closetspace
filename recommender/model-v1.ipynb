{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "942c105c-8c50-4b7f-a054-8f1c3b036d69",
   "metadata": {},
   "source": [
    "# This is v1 of the recommender system. \n",
    "\n",
    "It uses a pretrained ResNet50 model to encode images into a latent space, and performs ANNOY on the latent space representations of images to generate recommendations. The dataset used for recommendations is a custom dataset scraped from nordstrom.com and the input to the NN algorithm is the mean latent space representation of all items in a user's wardrobe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f742b75-260a-4ddf-9a49-11ff43134c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm\n",
    "!pip install sentence-transformers\n",
    "!pip install annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c13c11-3df3-427e-87bd-6190878e0b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5460f290-b55f-4acb-98f8-039dd137419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the encoder for images using ResNet50\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "feature_extractor = nn.Sequential(*(list(resnet50.children())[:-1])) # remove fc layer used for classification\n",
    "feature_extractor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1183a9f-eb0b-42b0-9d78-660b220f1630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_folder, transform):\n",
    "        self.image_folder = image_folder\n",
    "        self.image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder)]\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path)\n",
    "        return self.transform(image), img_path # return (image, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c48bed0b-446e-4688-b53b-6b0a46fe656f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# Define constants\n",
    "def convert_to_rgb(image):\n",
    "    # Convert RGBA or grayscale to RGB\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(convert_to_rgb),\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "batch_size=32\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "feature_extractor.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34fa4118-15f9-4bb0-9cb8-c96f89921a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_image_dataset(image_folder, save_to_file=False, filename=\"\"):\n",
    "    dataset = ImageDataset(image_folder, transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    latent_representations = {}\n",
    "    with torch.no_grad():\n",
    "        for images, paths in tqdm(dataloader, desc=\"Processing Images\", unit='batch'):\n",
    "            images = images.to(device) # Output: [batch_size, 3, 224, 224]\n",
    "            features = feature_extractor(images).squeeze() # Output: [batch_size, 2048]\n",
    "            for path, feature in zip(paths, features.cpu()):\n",
    "                latent_representations[path] = feature.numpy()\n",
    "    if save_to_file:\n",
    "        np.save(filename, latent_representations)\n",
    "\n",
    "    return latent_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115cb013-7a07-4ebd-8371-c0b46c79393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the fashion dataset and compute embeddings\n",
    "# NOTE: without a GPU, this cell could take hours to finish\n",
    "fd_image_folder = \"./fashion-dataset/images\"\n",
    "fd_lat_rep = embed_image_dataset(fd_iamge_folder, True, \"lat_rep_fd_nft.npy\") # save embeddings to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6160b210-8302-4b57-96a0-cfb75d8732ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.77batch/s]\n"
     ]
    }
   ],
   "source": [
    "# load the wardrobe dataset and compute embeddings\n",
    "wardrobe_folder = \"./sample-wardrobe/images\"\n",
    "wardrobe_lat_rep = embed_image_dataset(wardrobe_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3ca46a7-060f-4d41-b920-47472f98d01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████████████████████████████████████████████| 219/219 [10:44<00:00,  2.94s/batch]\n"
     ]
    }
   ],
   "source": [
    "# load the inventory (nordstrom) dataset and compute embeddings\n",
    "inventory_folder = \"./nordstrom-data/images\"\n",
    "inventory_lat_rep = embed_image_dataset(inventory_folder, True, \"lat_rep_inventory_nft.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9ee279b0-99a5-44ef-b8c6-ef93b7a001f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of fashion dataset embeddings and paths\n",
    "inventory_lat_rep = np.load(\"lat_rep_inventory_nft.npy\", allow_pickle=True).item()\n",
    "inventory_img_paths = list(inventory_lat_rep.keys())\n",
    "inventory_features = np.array(list(inventory_lat_rep.values()))\n",
    "\n",
    "# create list of wardrobe embeddings and paths\n",
    "wardrobe_paths = list(wardrobe_lat_rep.keys())\n",
    "wardrobe_features = np.array(list(wardrobe_lat_rep.values()))\n",
    "\n",
    "# get the mean embedding of all items in wardrobe\n",
    "mean_embedding = np.mean(wardrobe_features, axis=0)\n",
    "\n",
    "# mean_embedding = wardrobe_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4fc48cbb-2bcd-4b67-ad71-5467c468684e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended indices: [430, 2302, 1373, 5419, 6865, 1326, 2682, 1513, 3454, 6030]\n"
     ]
    }
   ],
   "source": [
    "# Perform Annoy\n",
    "embedding_dim = 2048  # Original dimensionality\n",
    "annoy_index = AnnoyIndex(embedding_dim, metric='euclidean')\n",
    "\n",
    "# Add all items to Annoy index\n",
    "for i, embedding in enumerate(inventory_features):\n",
    "    annoy_index.add_item(i, embedding)\n",
    "\n",
    "# Build the index\n",
    "n_trees = 50\n",
    "annoy_index.build(n_trees)  # Number of trees\n",
    "\n",
    "# Query the index\n",
    "n_neighbors = 10\n",
    "indices = annoy_index.get_nns_by_vector(mean_embedding, n_neighbors, include_distances=True)\n",
    "\n",
    "print(\"Recommended indices:\", indices[0])\n",
    "for idx in indices[0]:\n",
    "    im = Image.open(inventory_img_paths[idx])\n",
    "    im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2b9b2b-57ce-4ebc-8d33-e6a8b35606ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
