{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1687b67-468e-4ff6-a63f-84d925badfc7",
   "metadata": {},
   "source": [
    "# This is v3 of the recommender system.\n",
    "\n",
    "To improve on v2, we add distributed computing support with Spark and Petastorm to ingest training data stored in Amazon S3 at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d0c9e9-71f3-40b3-8dd2-2507b4482d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm\n",
    "!pip install sentence-transformers\n",
    "!pip install annoy\n",
    "!pip install torchsummary\n",
    "!pip install pyspark\n",
    "!pip install petastorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8c13c11-3df3-427e-87bd-6190878e0b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from annoy import AnnoyIndex\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b726a5b-d0b6-4ce4-85f4-1c350bda55cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/03 21:04:12 WARN Utils: Your hostname, Tylers-MacBook-Pro-5.local resolves to a loopback address: 127.0.0.1; using 192.168.10.138 instead (on interface en0)\n",
      "25/02/03 21:04:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/03 21:04:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# create spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.version\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3d2f559-638a-4c33-a394-173cc420fb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "accessKeyId = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "secretAccessKey = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.access.key', accessKeyId)\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.secret.key', secretAccessKey)\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.path.style.access', 'true')\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    " \n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.endpoint', 's3.amazonaws.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d233b5c-2c91-4441-ab87-6c22b519e8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pkill -f pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369ae967-7ee1-4190-a7c4-ff021d9d8911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a metadata file for fetching images from s3 and store in paraquet format on s3\n",
    "# metadata contains s3_path, + everything in styles.csv\n",
    "df = spark.read.csv(\"s3a://closetspace/train_data/metadata/styles.csv\", header=True, inferSchema=True)\n",
    "# add the s3_path to the df\n",
    "s3_image_prefix = \"s3a://closetspace/train_data/images/\"\n",
    "df = df.withColumn('s3_path', F.concat(F.lit(s3_image_prefix), df.id, F.lit('.jpg')))\n",
    "df.show()\n",
    "\n",
    "# write the full metadata df to s3a://closetspace/train_data/metadata/full_metadata.paraquet\n",
    "df.write.mode(\"overwrite\").parquet(\"s3a://closetspace/train_data/metadata/full_metadata.paraquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086b5319-023d-4d50-8444-7debc778a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing_read = spark.read.parquet(\"s3a://closetspace/train_data/metadata/full_metadata.paraquet\")\n",
    "df_testing_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c6c31a-eaf5-4c4c-a755-d3dd9402682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train, val, and test splits, saving each as their own paraquet file on s3\n",
    "# Split the data: 80% train, 10% validation, 10% test\n",
    "seed = 21\n",
    "train_df, temp_df = df.randomSplit([0.8, 0.2], seed=seed)\n",
    "val_df, test_df = temp_df.randomSplit([0.5, 0.5], seed=seed)\n",
    "\n",
    "train_df.write.mode(\"overwrite\").parquet(\"s3a://closetspace/train_data/metadata/train.paraquet\")\n",
    "val_df.write.mode(\"overwrite\").parquet(\"s3a://closetspace/train_data/metadata/val.paraquet\")\n",
    "test_df.write.mode(\"overwrite\").parquet(\"s3a://closetspace/train_data/metadata/test.paraquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48bed0b-446e-4688-b53b-6b0a46fe656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "def convert_to_rgb(image):\n",
    "    # Convert RGBA or grayscale to RGB\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(convert_to_rgb),\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "batch_size=32\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b78999e-fb71-42b0-b424-99caab333ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pyarrow petastorm\n",
    "!pip install -U s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0507720c-9dd6-4fe4-bf55-af26cfd1d801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataloader using Petastorm (this seems deprecated)\n",
    "import petastorm\n",
    "from petastorm import make_reader\n",
    "from petastorm.pytorch import DataLoader # 2 imports to dataloader?\n",
    "import boto3\n",
    "import io\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "def load_data(sample):\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    bucket, key = sample[\"s3_path\"].replace(\"s3a://\", \"\").split(\"/\", 1)\n",
    "\n",
    "    # read image from S3\n",
    "    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    image = Image.open(io.BytesIO(response[\"Body\"].read())).convert(\"RGB\")\n",
    "\n",
    "    # apply transform for ResNet50\n",
    "    image_tensor = transform(image)\n",
    "\n",
    "    description = sample[\"productDisplayName\"]\n",
    "\n",
    "    return image_tensor, description\n",
    "\n",
    "train_reader = make_reader(\"s3a://closetspace/train_data/metadata/train.parquet\", reader_pool_type=\"thread\", shuffle=True)\n",
    "val_reader = make_reader(\"s3a://closetspace/train_data/metadata/val.parquet\", reader_pool_type=\"thread\")\n",
    "test_reader = make_reader(\"s3a://closetspace/train_data/metadata/test.parquet\", reader_pool_type=\"thread\")\n",
    "\n",
    "train_dataloader = DataLoader(train_reader, batch_size=32, transform=load_data)\n",
    "val_dataloader = DataLoader(val_reader, batch_size=32, transform=load_data)\n",
    "test_dataloader = DataLoader(test_reader, batch_size=32, transform=load_data)\n",
    "\n",
    "# then, train as usual! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cf27c9-a4e0-4072-992c-cbf1047a3d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0162e3-2705-4705-8ce3-cb3702a5871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace above cell with aws s3 connector\n",
    "import boto3\n",
    "import io\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "\n",
    "# S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "response = s3_client.list_buckets()\n",
    "\n",
    "print(response['Buckets'])\n",
    "\n",
    "bucket_name=\"closetspace\"\n",
    "response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=\"train_data/metadata\")\n",
    "\n",
    "for obj in response.get('Contents', []):\n",
    "    print(obj['Key'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "56ad97e2-0699-4f09-8813-cead92bdaa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "# the commented out code is the implementation for no spark\n",
    "class S3ImageDataset(Dataset):\n",
    "    def __init__(self, parquet_path, bucket_name, transform=None):\n",
    "        self.bucket_name = bucket_name\n",
    "        self.parquet_path = parquet_path\n",
    "        self.transform = transform\n",
    "        self.metadata_df = self.load_metadata_from_s3(parquet_path)\n",
    "\n",
    "    # def load_metadata_from_s3(self, s3_path):\n",
    "    #     bucket_name, prefix = s3_path.replace(\"s3a://\", \"\").split(\"/\", 1)\n",
    "        \n",
    "    #     # list all the parquet files in the S3 directory\n",
    "    #     response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    #     files = [content['Key'] for content in response.get('Contents', []) if content['Key'].endswith('.parquet')]\n",
    "        \n",
    "    #     # read all the paraquet files in Paraquet folder\n",
    "    #     dfs = []\n",
    "    #     for file_key in files:\n",
    "    #         response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    #         parquet_data = response['Body'].read()\n",
    "    #         df = pd.read_parquet(io.BytesIO(parquet_data))\n",
    "    #         dfs.append(df)\n",
    "\n",
    "    #     # Concatenate all DataFrames (all part files) into one DataFrame\n",
    "    #     full_metadata_df = pd.concat(dfs, ignore_index=True)\n",
    "    #     return full_metadata_df\n",
    "\n",
    "    def load_metadata_from_s3(self, s3_path):\n",
    "        bucket_name, prefix = s3_path.replace(\"s3a://\", \"\").split(\"/\", 1)\n",
    "        response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "        \n",
    "        # List all Parquet files in the S3 directory\n",
    "        files = [content['Key'] for content in response.get('Contents', []) if content['Key'].endswith('.parquet')]\n",
    "        \n",
    "        # Use Spark to read all parquet files into a single DataFrame\n",
    "        if files:\n",
    "            s3_paths = [f\"s3a://{bucket_name}/{file_key}\" for file_key in files]\n",
    "            \n",
    "            # Read all Parquet files into a single Spark DataFrame\n",
    "            metadata_df = spark.read.parquet(*s3_paths)\n",
    "        else:\n",
    "            metadata_df = spark.createDataFrame([], schema=None)\n",
    "\n",
    "        metadata_df = metadata_df.withColumn(\"index_column\", F.monotonically_increasing_id())\n",
    "        return metadata_df\n",
    "\n",
    "    # def __len__(self):\n",
    "    #     return len(self.metadata_df)\n",
    "    def __len__(self):\n",
    "        return self.metadata_df.count()\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     s3_path = self.metadata_df.iloc[idx]['s3_path']\n",
    "    #     description = self.metadata_df.iloc[idx]['productDisplayName']\n",
    "\n",
    "    #     bucket_name, file_key = s3_path.replace(\"s3a://\", \"\").split(\"/\", 1)\n",
    "        \n",
    "    #     # read the image from S3\n",
    "    #     response = s3_client.get_object(Bucket=self.bucket_name, Key=file_key)\n",
    "    #     image = Image.open(io.BytesIO(response['Body'].read())).convert(\"RGB\")\n",
    "        \n",
    "    #     if self.transform:\n",
    "    #         image = self.transform(image)\n",
    "\n",
    "    #     return image, description\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata_df.filter(F.col('index_column') == idx).collect()\n",
    "\n",
    "        s3_path = row[0]['s3_path']\n",
    "        description = row[0]['productDisplayName']\n",
    "        \n",
    "        bucket_name, file_key = s3_path.replace(\"s3a://\", \"\").split(\"/\", 1)\n",
    "\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "        image_data = response['Body'].read()\n",
    "\n",
    "        image = Image.open(io.BytesIO(image_data))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, description\n",
    "\n",
    "bucket_name = \"closetspace\"\n",
    "\n",
    "train_path = \"s3a://closetspace/train_data/metadata/train.paraquet\"\n",
    "val_path = \"s3a://closetspace/train_data/metadata/val.paraquet\"\n",
    "test_path = \"s3a://closetspace/train_data/metadata/test.paraquet\"\n",
    "\n",
    "train_dataset = S3ImageDataset(parquet_path, bucket_name, transform)\n",
    "val_dataset = S3ImageDataset(parquet_path, bucket_name, transform)\n",
    "test_dataset = S3ImageDataset(parquet_path, bucket_name, transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f73973d-9833-4186-a5df-25376191c9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "('Turtle Check Men Navy Blue Shirt', 'Peter England Men Party Blue Jeans', 'Titan Women Silver Watch', 'Manchester United Men Solid Black Track Pants', 'Puma Men Grey T-shirt', 'Inkfruit Mens Chain Reaction T-shirt', 'Fabindia Men Striped Green Shirt', 'Jealous 21 Women Purple Shirt', 'Puma Men Pack of 3 Socks', 'Skagen Men Black Watch', 'Puma Men Future Cat Remix SF Black Casual Shoes', 'Fossil Women Black Huarache Weave Belt', 'Fila Men Cush Flex Black Slippers', 'Murcia Women Blue Handbag', 'Ben 10 Boys Navy Blue Slippers', 'Reid & Taylor Men Check Purple Shirts', 'Police Men Black Dial Watch PL12889JVSB', 'Gini and Jony Girls Knit White Top', 'Bwitch Beige Full-Coverage Bra BW335', 'Baggit Women Brown Handbag', 'CASIO G-Shock Men Black Digital Watch G-7710-1DR G223', 'ADIDAS Men Spry M Black Sandals', 'Timberland Unisex Rubber Sole Brush Shoe Accessories', 'ADIDAS Men Lfc Auth Hood Grey Sweatshirts', 'David Beckham Signature Men Deos', 'Buckaroo Men Flores Black Formal Shoes', 'Pitaraa Women Bronze Beaded Bracelet', \"Fila Men's Round Neck Navy Blue T-shirt\", 'Colorbar Soft Touch Show Stopper Copper Lipstick 037', 'Murcia Women Casual Brown Handbag', 'John Players Men Navy Blue Shirt', \"Disney Kids Boy's Crew Sea Life Sialing Green Teen Kidswear\")\n"
     ]
    }
   ],
   "source": [
    "for images, descriptions in test_dataloader:\n",
    "    print(images.shape)\n",
    "    print(descriptions)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1183a9f-eb0b-42b0-9d78-660b220f1630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path)\n",
    "        return self.transform(image), img_path # return (image, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce92f659-0ba2-4d68-9200-17ec243dbcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        resnet50 = models.resnet50(pretrained=True)\n",
    "        self.encoder = nn.Sequential(*(list(resnet50.children())[:-1])) # remove fc layer used for classification\n",
    "\n",
    "        # freeze layers up to 3 to retain information learned from pretrained weights\n",
    "        for name, layer in self.encoder.named_children():\n",
    "            if name in['0', '1', '2', '3']:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x).view(x.size(0), -1)\n",
    "        return latent\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=2048):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            # Fully connected layer to expand the latent vector\n",
    "            nn.Linear(latent_dim, 8 * 8 * 256),  # 8x8 spatial dimension and 256 channels\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(8 * 8 * 256),\n",
    "            \n",
    "            # Reshape to (B, 256, 8, 8) via view\n",
    "            nn.Unflatten(1, (256, 8, 8)),\n",
    "            \n",
    "            # Upsampling layers (transpose convolutions)\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=2),    # 8x8 -> 14x14\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),     # 14x14 -> 28x28\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),      # 28x28 -> 56x56\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),      # 56x56 -> 112x112\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(16, 3, kernel_size=4, stride=2, padding=1),       # 112x112 -> 224x224\n",
    "            nn.Sigmoid()  # Scaling the output to [0, 1] for RGB images\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder(latent_dim=2048)\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return latent, reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4786d5d-fed9-45bf-8b7a-ee4ad7bef6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Autoencoder().decoder\n",
    "summary(decoder, (2048,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b0eed1-2999-481d-97da-d27d5a6901fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "# Train, val, test splits (60, 20, 20)\n",
    "image_folder = \"./fashion-dataset/images\"\n",
    "image_paths = [os.path.join(image_folder, fname) for fname in os.listdir(image_folder)]\n",
    "\n",
    "train_val_paths, test_paths = train_test_split(image_paths, test_size=0.2)\n",
    "train_paths, val_paths = train_test_split(train_val_paths, test_size=0.25)\n",
    "\n",
    "train_dataset = ImageDataset(train_paths, transform)\n",
    "val_dataset = ImageDataset(val_paths, transform)\n",
    "test_dataset = ImageDataset(test_paths, transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84891ef9-bb1e-45f2-99c0-99daca9a2356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    # Define the mean and std for un-normalization (match your transform normalization)\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(1, 3, 1, 1).to(device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225], device=device).view(1, 3, 1, 1).to(device)\n",
    "    \n",
    "    for images, paths in tqdm(dataloader, desc=\"Training loop\", unit='batch', leave=True):\n",
    "        images = images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        latent, reconstructed = model(images)\n",
    "\n",
    "        # Un-normalize the input images\n",
    "        unnormalized_images = images * std + mean\n",
    "        \n",
    "        loss = criterion(reconstructed, unnormalized_images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126361f5-bdce-496b-b988-eb99b99fdc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"resnet50_autoencoder.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc48354b-4fd2-4000-8e4f-ad0276c1280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder().to(device)\n",
    "model.load_state_dict(torch.load(\"resnet50_autoencoder.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fbf2a6-8623-41c5-9a92-32aeace20c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_loss(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    # Define the mean and std for un-normalization (match your transform normalization)\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(1, 3, 1, 1).to(device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225], device=device).view(1, 3, 1, 1).to(device)\n",
    "    for images, paths in tqdm(dataloader, desc=\"Evaluation loop\", unit='batch', leave=True):\n",
    "        images = images.to(device)\n",
    "        latent, reconstructed = model(images)\n",
    "\n",
    "        # Un-normalize the input images\n",
    "        unnormalized_images = images * std + mean\n",
    "        \n",
    "        loss = criterion(reconstructed, unnormalized_images)\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "val_loss = evaluate_model_loss(model, val_dataloader, criterion, device)\n",
    "print(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc60d222-c032-4823-b214-c5d7dba9c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_image_dataset(dataloader, model, device, save_to_file=False, filename=\"\"):\n",
    "    latent_representations = {}\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, paths in tqdm(dataloader, desc=\"Processing Images\", unit='batch'):\n",
    "            images = images.to(device) # Output: [batch_size, 3, 224, 224]\n",
    "            features = model(images)[0] # Output: [batch_size, 2048]\n",
    "            if features.size(0) > 1:\n",
    "                features = features.squeeze()\n",
    "            for path, feature in zip(paths, features.cpu()):\n",
    "                latent_representations[path] = feature.numpy()\n",
    "    if save_to_file:\n",
    "        np.save(filename, latent_representations)\n",
    "\n",
    "    return latent_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787d9d7a-1c53-4b4f-acd5-714b245cd29b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the wardrobe dataset and compute embeddings\n",
    "wardrobe_folder = \"./sample-wardrobe/images\"\n",
    "wardrobe_paths = [os.path.join(wardrobe_folder, fname) for fname in os.listdir(wardrobe_folder)]\n",
    "wardrobe_dataset = ImageDataset(wardrobe_paths, transform)\n",
    "wardrobe_dataloader = DataLoader(wardrobe_dataset, batch_size=batch_size, shuffle=False)\n",
    "wardrobe_lat_rep = embed_image_dataset(wardrobe_dataloader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b7220e-23ea-44ee-9c13-b0a1637d87a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_folder = \"./nordstrom-data/images\"\n",
    "inventory_paths = [os.path.join(inventory_folder, fname) for fname in os.listdir(inventory_folder)]\n",
    "inventory_dataset = ImageDataset(inventory_paths, transform)\n",
    "inventory_dataloader = DataLoader(inventory_dataset, batch_size=batch_size, shuffle=False)\n",
    "inventory_lat_rep = embed_image_dataset(inventory_dataloader, model, device, True, \"lat_rep_inventory_ft.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee279b0-99a5-44ef-b8c6-ef93b7a001f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of fashion dataset embeddings and paths\n",
    "inventory_lat_rep = np.load(\"lat_rep_inventory_ft.npy\", allow_pickle=True).item()\n",
    "inventory_img_paths = list(inventory_lat_rep.keys())\n",
    "inventory_features = np.array(list(inventory_lat_rep.values()))\n",
    "\n",
    "# create list of wardrobe embeddings and paths\n",
    "wardrobe_paths = list(wardrobe_lat_rep.keys())\n",
    "wardrobe_features = np.array(list(wardrobe_lat_rep.values()))\n",
    "\n",
    "# get the mean embedding of all items in wardrobe\n",
    "mean_embedding = np.mean(wardrobe_features, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc48cbb-2bcd-4b67-ad71-5467c468684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Annoy\n",
    "embedding_dim = 2048  # Original dimensionality\n",
    "annoy_index = AnnoyIndex(embedding_dim, metric='euclidean')\n",
    "\n",
    "# Add all items to Annoy index\n",
    "for i, embedding in enumerate(inventory_features):\n",
    "    annoy_index.add_item(i, embedding)\n",
    "\n",
    "# Build the index\n",
    "n_trees = 50\n",
    "annoy_index.build(n_trees)  # Number of trees\n",
    "\n",
    "# Query the index\n",
    "n_neighbors = 10\n",
    "indices = annoy_index.get_nns_by_vector(mean_embedding, n_neighbors, include_distances=True)\n",
    "\n",
    "print(\"Recommended indices:\", indices[0])\n",
    "for idx in indices[0]:\n",
    "    # print(fd_img_paths[idx])\n",
    "    im = Image.open(inventory_img_paths[idx])\n",
    "    im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca476301-2ac4-4bd8-afd4-558f1f3f0895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder for metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacf0241-4d9a-4c74-a96a-f0ec6b4d7a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95bc281-71c4-49c8-8763-e13ca2ea1454",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# example input: \n",
    "# metadata = [\"Nike\", \"Nike Pegasus 40 White/Black\"] # dim = d\n",
    "# embedding = model.encode(metadata) # shape = [d x 384]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eed24bb-d4d3-495b-b38a-d3c63d102cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_metadata_rep = {}\n",
    "formatted_products = {}\n",
    "with open('./nordstrom-data/nordstrom_data.json', 'r') as f:\n",
    "    formatted_products = json.load(f)\n",
    "\n",
    "for path in tqdm(inventory_img_paths):\n",
    "    base_path = os.path.basename(path)\n",
    "    product = formatted_products[base_path]\n",
    "    metadata = [product['brand'], product['name']]\n",
    "    inventory_metadata_rep[path] = np.array(model.encode(metadata)).flatten()\n",
    "\n",
    "np.save(\"lat_rep_inventory_metadata.npy\", inventory_metadata_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f8a522-78cc-4304-a713-6e01f256ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wardrobe_metadata_rep = {}\n",
    "wardrobe_products = {}\n",
    "with open(\"./sample-wardrobe/metadata.csv\", mode=\"r\") as file:\n",
    "    csv_reader = csv.DictReader(file)  # DictReader reads rows as dictionaries\n",
    "    for row in csv_reader:\n",
    "        src = row['filename']\n",
    "        wardrobe_products[src] = row\n",
    "\n",
    "for path in wardrobe_paths:\n",
    "    base_path = os.path.basename(path)\n",
    "    product = wardrobe_products[base_path]\n",
    "    metadata = [product['brand'], product['name']]\n",
    "    wardrobe_metadata_rep[path] = np.array(model.encode(metadata)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5020450-9ccb-4db6-98f1-859ba3cf0a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat embeddings\n",
    "inventory_concat_embedding = {}\n",
    "wardrobe_concat_embedding = {}\n",
    "\n",
    "for path in inventory_img_paths:\n",
    "    inventory_concat_embedding[path] = np.concatenate((inventory_lat_rep[path], inventory_metadata_rep[path]))\n",
    "\n",
    "for path in wardrobe_paths:\n",
    "    wardrobe_concat_embedding[path] = np.concatenate((wardrobe_lat_rep[path], wardrobe_metadata_rep[path]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593ae946-b042-4255-b6f1-1768c4133b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: save concatenated embeddings to file\n",
    "np.save(\"inventory_concat_embed_v2.npy\", inventory_concat_embedding)\n",
    "np.save(\"wardrobe_concat_embed_v2.npy\", wardrobe_concat_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569433c4-9f52-4d23-904c-c549c452c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_concat_embedding = np.load(\"inventory_concat_embed_v2.npy\", allow_pickle=True).item()\n",
    "wardrobe_concat_embedding = np.load(\"wardrobe_concat_embed_v2.npy\", allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a27b7d-a2dd-449f-9b38-6c6ae1e8d8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_img_paths = list(inventory_concat_embedding.keys())\n",
    "inventory_features = np.array(list(inventory_concat_embedding.values()))\n",
    "\n",
    "# create list of wardrobe embeddings and paths\n",
    "wardrobe_paths = list(wardrobe_concat_embedding.keys())\n",
    "wardrobe_features = np.array(list(wardrobe_concat_embedding.values()))\n",
    "\n",
    "# grab the mean embedding\n",
    "mean_embedding = np.mean(wardrobe_features, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dddc5e-8f04-49b9-979b-6b24cb28f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Annoy\n",
    "embedding_dim = 2816  # Original dimensionality\n",
    "annoy_index = AnnoyIndex(embedding_dim, metric='euclidean')\n",
    "\n",
    "# Add all items to Annoy index\n",
    "for i, embedding in enumerate(inventory_features):\n",
    "    annoy_index.add_item(i, embedding)\n",
    "\n",
    "# Build the index\n",
    "n_trees = 50\n",
    "annoy_index.build(n_trees)  # Number of trees\n",
    "\n",
    "# Query the index\n",
    "n_neighbors = 10\n",
    "indices = annoy_index.get_nns_by_vector(mean_embedding, n_neighbors, include_distances=True)\n",
    "\n",
    "for idx in indices[0]:\n",
    "    im = Image.open(inventory_img_paths[idx])\n",
    "    im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8c1ac3-7984-4dea-a0c7-e18ee59aa470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
