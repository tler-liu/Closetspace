{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1687b67-468e-4ff6-a63f-84d925badfc7",
   "metadata": {},
   "source": [
    "# This is v2 of the recommender system.\n",
    "\n",
    "To improve on v1, we fine tuned the ResNet50 model by training it to encode and reconstruct images from the fashion dataset (https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-dataset). On top of embedding images, we also embedded the metadata (name, brand) for products and factored that information into recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d0c9e9-71f3-40b3-8dd2-2507b4482d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm\n",
    "!pip install sentence-transformers\n",
    "!pip install annoy\n",
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c13c11-3df3-427e-87bd-6190878e0b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from annoy import AnnoyIndex\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1183a9f-eb0b-42b0-9d78-660b220f1630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path)\n",
    "        return self.transform(image), img_path # return (image, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce92f659-0ba2-4d68-9200-17ec243dbcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        resnet50 = models.resnet50(pretrained=True)\n",
    "        self.encoder = nn.Sequential(*(list(resnet50.children())[:-1])) # remove fc layer used for classification\n",
    "\n",
    "        # freeze layers up to 3 to retain information learned from pretrained weights\n",
    "        for name, layer in self.encoder.named_children():\n",
    "            if name in['0', '1', '2', '3']:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x).view(x.size(0), -1)\n",
    "        return latent\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=2048):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            # Fully connected layer to expand the latent vector\n",
    "            nn.Linear(latent_dim, 8 * 8 * 256),  # 8x8 spatial dimension and 256 channels\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(8 * 8 * 256),\n",
    "            \n",
    "            # Reshape to (B, 256, 8, 8) via view\n",
    "            nn.Unflatten(1, (256, 8, 8)),\n",
    "            \n",
    "            # Upsampling layers (transpose convolutions)\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=2),    # 8x8 -> 14x14\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),     # 14x14 -> 28x28\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),      # 28x28 -> 56x56\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),      # 56x56 -> 112x112\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(16, 3, kernel_size=4, stride=2, padding=1),       # 112x112 -> 224x224\n",
    "            nn.Sigmoid()  # Scaling the output to [0, 1] for RGB images\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder(latent_dim=2048)\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return latent, reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4786d5d-fed9-45bf-8b7a-ee4ad7bef6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Autoencoder().decoder\n",
    "summary(decoder, (2048,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48bed0b-446e-4688-b53b-6b0a46fe656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "def convert_to_rgb(image):\n",
    "    # Convert RGBA or grayscale to RGB\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(convert_to_rgb),\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "batch_size=32\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b0eed1-2999-481d-97da-d27d5a6901fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "# Train, val, test splits (60, 20, 20)\n",
    "image_folder = \"./fashion-dataset/images\"\n",
    "image_paths = [os.path.join(image_folder, fname) for fname in os.listdir(image_folder)]\n",
    "\n",
    "train_val_paths, test_paths = train_test_split(image_paths, test_size=0.2)\n",
    "train_paths, val_paths = train_test_split(train_val_paths, test_size=0.25)\n",
    "\n",
    "train_dataset = ImageDataset(train_paths, transform)\n",
    "val_dataset = ImageDataset(val_paths, transform)\n",
    "test_dataset = ImageDataset(test_paths, transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84891ef9-bb1e-45f2-99c0-99daca9a2356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    # Define the mean and std for un-normalization (match your transform normalization)\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(1, 3, 1, 1).to(device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225], device=device).view(1, 3, 1, 1).to(device)\n",
    "    \n",
    "    for images, paths in tqdm(dataloader, desc=\"Training loop\", unit='batch', leave=True):\n",
    "        images = images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        latent, reconstructed = model(images)\n",
    "\n",
    "        # Un-normalize the input images\n",
    "        unnormalized_images = images * std + mean\n",
    "        \n",
    "        loss = criterion(reconstructed, unnormalized_images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126361f5-bdce-496b-b988-eb99b99fdc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"resnet50_autoencoder.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc48354b-4fd2-4000-8e4f-ad0276c1280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder().to(device)\n",
    "model.load_state_dict(torch.load(\"resnet50_autoencoder.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fbf2a6-8623-41c5-9a92-32aeace20c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_loss(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    # Define the mean and std for un-normalization (match your transform normalization)\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(1, 3, 1, 1).to(device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225], device=device).view(1, 3, 1, 1).to(device)\n",
    "    for images, paths in tqdm(dataloader, desc=\"Evaluation loop\", unit='batch', leave=True):\n",
    "        images = images.to(device)\n",
    "        latent, reconstructed = model(images)\n",
    "\n",
    "        # Un-normalize the input images\n",
    "        unnormalized_images = images * std + mean\n",
    "        \n",
    "        loss = criterion(reconstructed, unnormalized_images)\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "val_loss = evaluate_model_loss(model, val_dataloader, criterion, device)\n",
    "print(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc60d222-c032-4823-b214-c5d7dba9c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_image_dataset(dataloader, model, device, save_to_file=False, filename=\"\"):\n",
    "    latent_representations = {}\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, paths in tqdm(dataloader, desc=\"Processing Images\", unit='batch'):\n",
    "            images = images.to(device) # Output: [batch_size, 3, 224, 224]\n",
    "            features = model(images)[0].squeeze() # Output: [batch_size, 2048]\n",
    "            for path, feature in zip(paths, features.cpu()):\n",
    "                latent_representations[path] = feature.numpy()\n",
    "    if save_to_file:\n",
    "        np.save(filename, latent_representations)\n",
    "\n",
    "    return latent_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787d9d7a-1c53-4b4f-acd5-714b245cd29b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the wardrobe dataset and compute embeddings\n",
    "wardrobe_folder = \"./sample-wardrobe/images\"\n",
    "wardrobe_paths = [os.path.join(wardrobe_folder, fname) for fname in os.listdir(wardrobe_folder)]\n",
    "wardrobe_dataset = ImageDataset(wardrobe_paths, transform)\n",
    "wardrobe_dataloader = DataLoader(wardrobe_dataset, batch_size=batch_size, shuffle=False)\n",
    "wardrobe_lat_rep = embed_image_dataset(wardrobe_dataloader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b7220e-23ea-44ee-9c13-b0a1637d87a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_folder = \"./nordstrom-data/images\"\n",
    "inventory_paths = [os.path.join(inventory_folder, fname) for fname in os.listdir(inventory_folder)]\n",
    "inventory_dataset = ImageDataset(inventory_paths, transform)\n",
    "inventory_dataloader = DataLoader(inventory_dataset, batch_size=batch_size, shuffle=False)\n",
    "inventory_lat_rep = embed_image_dataset(inventory_dataloader, model, device, True, \"lat_rep_inventory_ft.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee279b0-99a5-44ef-b8c6-ef93b7a001f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of fashion dataset embeddings and paths\n",
    "inventory_lat_rep = np.load(\"lat_rep_inventory_ft.npy\", allow_pickle=True).item()\n",
    "inventory_img_paths = list(inventory_lat_rep.keys())\n",
    "inventory_features = np.array(list(inventory_lat_rep.values()))\n",
    "\n",
    "# create list of wardrobe embeddings and paths\n",
    "wardrobe_paths = list(wardrobe_lat_rep.keys())\n",
    "wardrobe_features = np.array(list(wardrobe_lat_rep.values()))\n",
    "\n",
    "# get the mean embedding of all items in wardrobe\n",
    "mean_embedding = np.mean(wardrobe_features, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc48cbb-2bcd-4b67-ad71-5467c468684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Annoy\n",
    "embedding_dim = 2048  # Original dimensionality\n",
    "annoy_index = AnnoyIndex(embedding_dim, metric='euclidean')\n",
    "\n",
    "# Add all items to Annoy index\n",
    "for i, embedding in enumerate(inventory_features):\n",
    "    annoy_index.add_item(i, embedding)\n",
    "\n",
    "# Build the index\n",
    "n_trees = 50\n",
    "annoy_index.build(n_trees)  # Number of trees\n",
    "\n",
    "# Query the index\n",
    "n_neighbors = 10\n",
    "indices = annoy_index.get_nns_by_vector(mean_embedding, n_neighbors, include_distances=True)\n",
    "\n",
    "print(\"Recommended indices:\", indices[0])\n",
    "for idx in indices[0]:\n",
    "    # print(fd_img_paths[idx])\n",
    "    im = Image.open(inventory_img_paths[idx])\n",
    "    im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca476301-2ac4-4bd8-afd4-558f1f3f0895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder for metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacf0241-4d9a-4c74-a96a-f0ec6b4d7a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95bc281-71c4-49c8-8763-e13ca2ea1454",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# example input: \n",
    "# metadata = [\"Nike\", \"Nike Pegasus 40 White/Black\"] # dim = d\n",
    "# embedding = model.encode(metadata) # shape = [d x 384]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eed24bb-d4d3-495b-b38a-d3c63d102cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_metadata_rep = {}\n",
    "formatted_products = {}\n",
    "with open('./nordstrom-data/nordstrom_data.json', 'r') as f:\n",
    "    formatted_products = json.load(f)\n",
    "\n",
    "for path in tqdm(inventory_img_paths):\n",
    "    base_path = os.path.basename(path)\n",
    "    product = formatted_products[base_path]\n",
    "    metadata = [product['brand'], product['name']]\n",
    "    inventory_metadata_rep[path] = np.array(model.encode(metadata)).flatten()\n",
    "\n",
    "np.save(\"lat_rep_inventory_metadata.npy\", inventory_metadata_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f8a522-78cc-4304-a713-6e01f256ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wardrobe_metadata_rep = {}\n",
    "wardrobe_products = {}\n",
    "with open(\"./sample-wardrobe/metadata.csv\", mode=\"r\") as file:\n",
    "    csv_reader = csv.DictReader(file)  # DictReader reads rows as dictionaries\n",
    "    for row in csv_reader:\n",
    "        src = row['filename']\n",
    "        wardrobe_products[src] = row\n",
    "\n",
    "for path in wardrobe_paths:\n",
    "    base_path = os.path.basename(path)\n",
    "    product = wardrobe_products[base_path]\n",
    "    metadata = [product['brand'], product['name']]\n",
    "    wardrobe_metadata_rep[path] = np.array(model.encode(metadata)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5020450-9ccb-4db6-98f1-859ba3cf0a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat embeddings\n",
    "inventory_concat_embedding = {}\n",
    "wardrobe_concat_embedding = {}\n",
    "\n",
    "for path in inventory_img_paths:\n",
    "    inventory_concat_embedding[path] = np.concatenate((inventory_lat_rep[path], inventory_metadata_rep[path]))\n",
    "\n",
    "for path in wardrobe_paths:\n",
    "    wardrobe_concat_embedding[path] = np.concatenate((wardrobe_lat_rep[path], wardrobe_metadata_rep[path]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593ae946-b042-4255-b6f1-1768c4133b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: save concatenated embeddings to file\n",
    "np.save(\"inventory_concat_embed_v2.npy\", inventory_concat_embedding)\n",
    "np.save(\"wardrobe_concat_embed_v2.npy\", wardrobe_concat_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569433c4-9f52-4d23-904c-c549c452c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_concat_embedding = np.load(\"inventory_concat_embed_v2.npy\", allow_pickle=True).item()\n",
    "wardrobe_concat_embedding = np.load(\"wardrobe_concat_embed_v2.npy\", allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a27b7d-a2dd-449f-9b38-6c6ae1e8d8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_img_paths = list(inventory_concat_embedding.keys())\n",
    "inventory_features = np.array(list(inventory_concat_embedding.values()))\n",
    "\n",
    "# create list of wardrobe embeddings and paths\n",
    "wardrobe_paths = list(wardrobe_concat_embedding.keys())\n",
    "wardrobe_features = np.array(list(wardrobe_concat_embedding.values()))\n",
    "\n",
    "# grab the mean embedding\n",
    "mean_embedding = np.mean(wardrobe_features, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dddc5e-8f04-49b9-979b-6b24cb28f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Annoy\n",
    "embedding_dim = 2816  # Original dimensionality\n",
    "annoy_index = AnnoyIndex(embedding_dim, metric='euclidean')\n",
    "\n",
    "# Add all items to Annoy index\n",
    "for i, embedding in enumerate(inventory_features):\n",
    "    annoy_index.add_item(i, embedding)\n",
    "\n",
    "# Build the index\n",
    "n_trees = 50\n",
    "annoy_index.build(n_trees)  # Number of trees\n",
    "\n",
    "# Query the index\n",
    "n_neighbors = 10\n",
    "indices = annoy_index.get_nns_by_vector(mean_embedding, n_neighbors, include_distances=True)\n",
    "\n",
    "for idx in indices[0]:\n",
    "    im = Image.open(inventory_img_paths[idx])\n",
    "    im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8c1ac3-7984-4dea-a0c7-e18ee59aa470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
