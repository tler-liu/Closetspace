{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d0c9e9-71f3-40b3-8dd2-2507b4482d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm\n",
    "!pip install sentence-transformers\n",
    "!pip install annoy\n",
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a8c13c11-3df3-427e-87bd-6190878e0b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from annoy import AnnoyIndex\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e1183a9f-eb0b-42b0-9d78-660b220f1630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path)\n",
    "        return self.transform(image), img_path # return (image, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce92f659-0ba2-4d68-9200-17ec243dbcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        resnet50 = models.resnet50(pretrained=True)\n",
    "        self.encoder = nn.Sequential(*(list(resnet50.children())[:-1])) # remove fc layer used for classification\n",
    "\n",
    "        # freeze layers up to 3 to retain information learned from pretrained weights\n",
    "        for name, layer in self.encoder.named_children():\n",
    "            if name in['0', '1', '2', '3']:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x).view(x.size(0), -1)\n",
    "        return latent\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=2048):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            # Fully connected layer to expand the latent vector\n",
    "            nn.Linear(latent_dim, 8 * 8 * 256),  # 8x8 spatial dimension and 256 channels\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(8 * 8 * 256),\n",
    "            \n",
    "            # Reshape to (B, 256, 8, 8) via view\n",
    "            nn.Unflatten(1, (256, 8, 8)),\n",
    "            \n",
    "            # Upsampling layers (transpose convolutions)\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=2),    # 8x8 -> 14x14\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),     # 14x14 -> 28x28\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),      # 28x28 -> 56x56\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),      # 56x56 -> 112x112\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(16, 3, kernel_size=4, stride=2, padding=1),       # 112x112 -> 224x224\n",
    "            nn.Sigmoid()  # Scaling the output to [0, 1] for RGB images\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder(latent_dim=2048)\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return latent, reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e4786d5d-fed9-45bf-8b7a-ee4ad7bef6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                [-1, 16384]      33,570,816\n",
      "              ReLU-2                [-1, 16384]               0\n",
      "       BatchNorm1d-3                [-1, 16384]          32,768\n",
      "         Unflatten-4            [-1, 256, 8, 8]               0\n",
      "   ConvTranspose2d-5          [-1, 128, 14, 14]         524,416\n",
      "              ReLU-6          [-1, 128, 14, 14]               0\n",
      "   ConvTranspose2d-7           [-1, 64, 28, 28]         131,136\n",
      "              ReLU-8           [-1, 64, 28, 28]               0\n",
      "   ConvTranspose2d-9           [-1, 32, 56, 56]          32,800\n",
      "             ReLU-10           [-1, 32, 56, 56]               0\n",
      "  ConvTranspose2d-11         [-1, 16, 112, 112]           8,208\n",
      "             ReLU-12         [-1, 16, 112, 112]               0\n",
      "  ConvTranspose2d-13          [-1, 3, 224, 224]             771\n",
      "          Sigmoid-14          [-1, 3, 224, 224]               0\n",
      "================================================================\n",
      "Total params: 34,300,915\n",
      "Trainable params: 34,300,915\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 8.54\n",
      "Params size (MB): 130.85\n",
      "Estimated Total Size (MB): 139.39\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "decoder = Autoencoder().decoder\n",
    "summary(decoder, (2048,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c48bed0b-446e-4688-b53b-6b0a46fe656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "batch_size=32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "64b0eed1-2999-481d-97da-d27d5a6901fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "# Train, val, test splits (60, 20, 20)\n",
    "image_folder = \"./fashion-dataset/images\"\n",
    "image_paths = [os.path.join(image_folder, fname) for fname in os.listdir(image_folder)]\n",
    "\n",
    "train_val_paths, test_paths = train_test_split(image_paths, test_size=0.2)\n",
    "train_paths, val_paths = train_test_split(train_val_paths, test_size=0.25)\n",
    "\n",
    "train_dataset = ImageDataset(train_paths, transform)\n",
    "val_dataset = ImageDataset(val_paths, transform)\n",
    "test_dataset = ImageDataset(test_paths, transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84891ef9-bb1e-45f2-99c0-99daca9a2356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop:   2%|â–Š                                                    | 13/834 [01:58<2:02:25,  8.95s/batch]"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, paths in tqdm(dataloader, desc=\"Training loop\", unit='batch', leave=True):\n",
    "        images = images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        latent, reconstructed = model(images)\n",
    "        loss = criterion(reconstructed, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"resnet50_autoencoder.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fa4118-15f9-4bb0-9cb8-c96f89921a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_image_dataset(image_folder, save_to_file=False, filename=\"\"):\n",
    "    dataset = ImageDataset(image_folder, transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    latent_representations = {}\n",
    "    with torch.no_grad():\n",
    "        for images, paths in tqdm(dataloader, desc=\"Processing Images\", unit='batch'):\n",
    "            images = images.to(device) # Output: [batch_size, 3, 224, 224]\n",
    "            features = feature_extractor(images).squeeze() # Output: [batch_size, 2048]\n",
    "            for path, feature in zip(paths, features.cpu()):\n",
    "                latent_representations[path] = feature.numpy()\n",
    "    if save_to_file:\n",
    "        np.save(filename, latent_representations)\n",
    "\n",
    "    return latent_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115cb013-7a07-4ebd-8371-c0b46c79393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the fashion dataset and compute embeddings\n",
    "fd_image_folder = \"./fashion-dataset/images\"\n",
    "fd_lat_rep = embed_image_dataset(fd_iamge_folder, True, \"lat_rep_fd_nft.npy\") # save embeddings to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6160b210-8302-4b57-96a0-cfb75d8732ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the wardrobe dataset and compute embeddings\n",
    "wardrobe_folder = \"./sample-wardrobe\"\n",
    "wardrobe_lat_rep = embed_image_dataset(wardrobe_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee279b0-99a5-44ef-b8c6-ef93b7a001f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of fashion dataset embeddings and paths\n",
    "latent_fd_images = np.load(\"lat_rep_fd_nft.npy\", allow_pickle=True).item()\n",
    "fd_img_paths = list(latent_fd_images.keys())\n",
    "fd_features = np.array(list(latent_fd_images.values())) # Output: (44441, 2048)\n",
    "\n",
    "# create list of wardrobe embeddings and paths\n",
    "wardrobe_paths = list(wardrobe_lat_rep.keys())\n",
    "wardrobe_features = np.array(list(wardrobe_lat_rep.values()))\n",
    "\n",
    "# get the mean embedding of all items in wardrobe\n",
    "mean_embedding = np.mean(wardrobe_features, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc48cbb-2bcd-4b67-ad71-5467c468684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Annoy\n",
    "embedding_dim = 2048  # Original dimensionality\n",
    "annoy_index = AnnoyIndex(embedding_dim, metric='euclidean')\n",
    "\n",
    "# Add all items to Annoy index\n",
    "for i, embedding in enumerate(fd_features):\n",
    "    annoy_index.add_item(i, embedding)\n",
    "\n",
    "# Build the index\n",
    "n_trees = 50\n",
    "annoy_index.build(n_trees)  # Number of trees\n",
    "\n",
    "# Query the index\n",
    "n_neighbors = 10\n",
    "indices = annoy_index.get_nns_by_vector(mean_embedding, n_neighbors, include_distances=True)\n",
    "\n",
    "print(\"Recommended indices:\", indices[0])\n",
    "for idx in indices[0]:\n",
    "    print(fd_img_paths[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18abed0e-769d-4117-b49e-9158526039a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(\"./fashion-dataset/images/52122.jpg\")\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ebea2b-b568-4e20-ac9b-9aaca40f7cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fine tune resnet50 model on fashion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca476301-2ac4-4bd8-afd4-558f1f3f0895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the encoder for metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacf0241-4d9a-4c74-a96a-f0ec6b4d7a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "metadata = [\"Nike\", \"Nike Pegasus 40 White/Black\"] # dim = d\n",
    "embedding = model.encode(metadata) # shape = [d x 384]\n",
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5020450-9ccb-4db6-98f1-859ba3cf0a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dddc5e-8f04-49b9-979b-6b24cb28f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform ANNOY on weighted embedding X' and inventory embeddings Y_1, ... Y_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609e3c09-0cc8-40ca-a497-f9e69859a352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output recommendations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
