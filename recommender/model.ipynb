{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f742b75-260a-4ddf-9a49-11ff43134c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm\n",
    "!pip install sentence-transformers\n",
    "!pip install annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a8c13c11-3df3-427e-87bd-6190878e0b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5460f290-b55f-4acb-98f8-039dd137419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the encoder for images using ResNet50\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "feature_extractor = nn.Sequential(*(list(resnet50.children())[:-1])) # remove fc layer used for classification\n",
    "feature_extractor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e1183a9f-eb0b-42b0-9d78-660b220f1630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_folder, transform):\n",
    "        self.image_folder = image_folder\n",
    "        self.image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder)]\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path)\n",
    "        return self.transform(image), img_path # return (image, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48bed0b-446e-4688-b53b-6b0a46fe656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "batch_size=32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "feature_extractor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "34fa4118-15f9-4bb0-9cb8-c96f89921a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_image_dataset(image_folder, save_to_file=False, filename=\"\"):\n",
    "    dataset = ImageDataset(image_folder, transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    latent_representations = {}\n",
    "    with torch.no_grad():\n",
    "        for images, paths in tqdm(dataloader, desc=\"Processing Images\", unit='batch'):\n",
    "            images = images.to(device) # Output: [batch_size, 3, 224, 224]\n",
    "            features = feature_extractor(images).squeeze() # Output: [batch_size, 2048]\n",
    "            for path, feature in zip(paths, features.cpu()):\n",
    "                latent_representations[path] = feature.numpy()\n",
    "    if save_to_file:\n",
    "        np.save(filename, latent_representations)\n",
    "\n",
    "    return latent_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115cb013-7a07-4ebd-8371-c0b46c79393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the fashion dataset and compute embeddings\n",
    "fd_image_folder = \"./fashion-dataset/images\"\n",
    "fd_lat_rep = embed_image_dataset(fd_iamge_folder, True, \"lat_rep_fd_nft.npy\") # save embeddings to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6160b210-8302-4b57-96a0-cfb75d8732ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the wardrobe dataset and compute embeddings\n",
    "wardrobe_folder = \"./sample-wardrobe\"\n",
    "wardrobe_lat_rep = embed_image_dataset(wardrobe_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9ee279b0-99a5-44ef-b8c6-ef93b7a001f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of fashion dataset embeddings and paths\n",
    "latent_fd_images = np.load(\"lat_rep_fd_nft.npy\", allow_pickle=True).item()\n",
    "fd_img_paths = list(latent_fd_images.keys())\n",
    "fd_features = np.array(list(latent_fd_images.values())) # Output: (44441, 2048)\n",
    "\n",
    "# create list of wardrobe embeddings and paths\n",
    "wardrobe_paths = list(wardrobe_lat_rep.keys())\n",
    "wardrobe_features = np.array(list(wardrobe_lat_rep.values()))\n",
    "\n",
    "# get the mean embedding of all items in wardrobe\n",
    "mean_embedding = np.mean(wardrobe_features, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4fc48cbb-2bcd-4b67-ad71-5467c468684e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended indices: [22758, 25579, 6241, 7707, 34351, 38216, 26620, 19066, 5721, 41925]\n",
      "./fashion-dataset/images/13326.jpg\n",
      "./fashion-dataset/images/13325.jpg\n",
      "./fashion-dataset/images/34044.jpg\n",
      "./fashion-dataset/images/34045.jpg\n",
      "./fashion-dataset/images/31090.jpg\n",
      "./fashion-dataset/images/40131.jpg\n",
      "./fashion-dataset/images/36205.jpg\n",
      "./fashion-dataset/images/35890.jpg\n",
      "./fashion-dataset/images/50717.jpg\n",
      "./fashion-dataset/images/52122.jpg\n"
     ]
    }
   ],
   "source": [
    "# Perform Annoy\n",
    "embedding_dim = 2048  # Original dimensionality\n",
    "annoy_index = AnnoyIndex(embedding_dim, metric='euclidean')\n",
    "\n",
    "# Add all items to Annoy index\n",
    "for i, embedding in enumerate(fd_features):\n",
    "    annoy_index.add_item(i, embedding)\n",
    "\n",
    "# Build the index\n",
    "n_trees = 50\n",
    "annoy_index.build(n_trees)  # Number of trees\n",
    "\n",
    "# Query the index\n",
    "n_neighbors = 10\n",
    "indices = annoy_index.get_nns_by_vector(mean_embedding, n_neighbors, include_distances=True)\n",
    "\n",
    "print(\"Recommended indices:\", indices[0])\n",
    "for idx in indices[0]:\n",
    "    print(fd_img_paths[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "18abed0e-769d-4117-b49e-9158526039a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(\"./fashion-dataset/images/52122.jpg\")\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ebea2b-b568-4e20-ac9b-9aaca40f7cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fine tune resnet50 model on fashion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ca476301-2ac4-4bd8-afd4-558f1f3f0895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the encoder for metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aacf0241-4d9a-4c74-a96a-f0ec6b4d7a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "metadata = [\"Nike\", \"Nike Pegasus 40 White/Black\"] # dim = d\n",
    "embedding = model.encode(metadata) # shape = [d x 384]\n",
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5020450-9ccb-4db6-98f1-859ba3cf0a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dddc5e-8f04-49b9-979b-6b24cb28f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform ANNOY on weighted embedding X' and inventory embeddings Y_1, ... Y_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609e3c09-0cc8-40ca-a497-f9e69859a352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output recommendations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
